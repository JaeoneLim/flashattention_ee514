# flashattention_ee514

Copyright belongs to Tri Dao's lab

# flash attention repository
https://github.com/Dao-AILab/flash-attention

# Installation
read ./flash_attention/README.md

# How to run benchmarks of flashattention2
cd ./flash-attention/benchmarks
python benchmark_flash_attention.py
(or python3 benchmark_flash_attention.py)

# our working files (cannot be compiled)
flash3_fwd_kernel.h
flash3_kernel_traits.h
